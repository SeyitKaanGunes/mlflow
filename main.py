"""
MLflow tracking demo built with a scikit-learn pipeline.

The script synthesises a small dataset that combines numerical, categorical,
and free-form text features. It trains a logistic-regression pipeline that
includes TF-IDF vectorisation for the text column and one-hot encoding for
categorical fields, evaluates the model, and logs the full experiment to MLflow.

Example:
    python main.py --samples 400 --experiment-name DemoRun

Requirements:
    pip install mlflow scikit-learn matplotlib pandas numpy
"""

from __future__ import annotations

import argparse
import json
import uuid
from pathlib import Path
from typing import Dict, Iterable

import matplotlib

# Use a non-interactive backend so the script works in headless environments.
matplotlib.use("Agg")

import matplotlib.pyplot as plt
import mlflow
import numpy as np
import pandas as pd
from joblib import dump
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


RANDOM_STATE_DEFAULT = 42
LABEL_COLUMN = "churn"
NUMERIC_FEATURES = ["age", "tenure_months", "monthly_spend"]
CATEGORICAL_FEATURES = ["region", "customer_segment", "favorite_product"]
TEXT_FEATURE = "interaction_summary"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Train a scikit-learn logistic-regression pipeline on a synthetic "
            "multimodal dataset and log the full run into MLflow."
        )
    )
    parser.add_argument(
        "--samples",
        type=int,
        default=400,
        help="Total number of synthetic samples to generate.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.25,
        help="Fraction of samples reserved for evaluation (0, 1).",
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default="mlflow-sklearn-demo",
        help="MLflow experiment name.",
    )
    parser.add_argument(
        "--tracking-uri",
        type=str,
        default=None,
        help="MLflow tracking URI. Defaults to local file store.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Custom MLflow run name. Defaults to autogenerated value.",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=RANDOM_STATE_DEFAULT,
        help="Global random seed used for reproducibility.",
    )
    parser.add_argument(
        "--artifact-dir",
        type=str,
        default="artifacts",
        help="Directory used to stash figures and reports before MLflow logging.",
    )
    parser.add_argument(
        "--tfidf-max-features",
        type=int,
        default=500,
        help="Maximum number of n-gram features extracted by the TF-IDF vectoriser.",
    )
    parser.add_argument(
        "--logreg-c",
        type=float,
        default=1.0,
        help="Inverse regularisation strength passed to LogisticRegression (higher is weaker regularisation).",
    )
    return parser.parse_args()


def ensure_reproducibility(seed: int) -> np.random.Generator:
    np.random.seed(seed)
    return np.random.default_rng(seed)


def synthesize_customer_data(
    num_samples: int,
    rng: np.random.Generator,
) -> pd.DataFrame:
    """Create a toy multimodal dataset with numerical, categorical, and text data."""
    age = rng.integers(18, 70, size=num_samples)
    tenure_months = rng.integers(1, 120, size=num_samples)
    monthly_spend = np.clip(rng.normal(loc=95, scale=25, size=num_samples), 10, None)

    regions = np.array(["North", "South", "East", "West", "Central"])
    customer_segments = np.array(["retail", "enterprise", "startup"])
    preferred_products = np.array(
        ["Analytics", "CRM", "Automation", "Security", "Integration"]
    )

    region = rng.choice(regions, size=num_samples, replace=True)
    segment = rng.choice(customer_segments, size=num_samples, replace=True)
    product = rng.choice(preferred_products, size=num_samples, replace=True)

    # Base propensity calculation encourages the model to pick up multimodal signals.
    spend_score = (110 - monthly_spend) / 45
    age_score = (35 - age) / 20
    tenure_penalty = (12 - tenure_months) / 18
    segment_bonus = np.isin(segment, ["startup"]).astype(float) * 0.6
    product_bonus = np.isin(product, ["Automation", "Integration"]).astype(float) * 0.5
    churn_logit = (
        -0.3 + spend_score + age_score + tenure_penalty + segment_bonus + product_bonus
    )

    probabilities = 1 / (1 + np.exp(-churn_logit))
    churn_flag = rng.random(num_samples) < probabilities

    text_description = [
        (
            f"{segment[idx].title()} customer from {region[idx]} region with "
            f"{tenure_months[idx]} months on contract spends roughly "
            f"${monthly_spend[idx]:.0f} per month and prioritizes {product[idx]}."
        )
        for idx in range(num_samples)
    ]

    df = pd.DataFrame(
        {
            "age": age,
            "tenure_months": tenure_months,
            "monthly_spend": monthly_spend,
            "region": region,
            "customer_segment": segment,
            "favorite_product": product,
            "interaction_summary": text_description,
            LABEL_COLUMN: np.where(churn_flag, "yes", "no"),
        }
    )
    return df


def prepare_train_test_split(
    df: pd.DataFrame,
    test_size: float,
    random_state: int,
) -> tuple[pd.DataFrame, pd.DataFrame]:
    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df[LABEL_COLUMN],
    )
    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)


def build_training_pipeline(
    tfidf_max_features: int,
    logreg_c: float,
    random_state: int,
) -> Pipeline:
    preprocessor = ColumnTransformer(
        transformers=[
            (
                "numeric",
                StandardScaler(with_mean=False),
                NUMERIC_FEATURES,
            ),
            (
                "categorical",
                OneHotEncoder(handle_unknown="ignore"),
                CATEGORICAL_FEATURES,
            ),
            (
                "text",
                TfidfVectorizer(
                    max_features=tfidf_max_features,
                    ngram_range=(1, 2),
                    min_df=2,
                ),
                TEXT_FEATURE,
            ),
        ]
    )

    classifier = LogisticRegression(
        C=logreg_c,
        max_iter=1000,
        random_state=random_state,
        solver="liblinear",
    )

    return Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", classifier),
        ]
    )


def train_model(
    train_data: pd.DataFrame,
    tfidf_max_features: int,
    logreg_c: float,
    random_state: int,
) -> tuple[Pipeline, Dict]:
    model = build_training_pipeline(
        tfidf_max_features=tfidf_max_features,
        logreg_c=logreg_c,
        random_state=random_state,
    )
    features = train_data.drop(columns=[LABEL_COLUMN])
    labels = train_data[LABEL_COLUMN]
    model.fit(features, labels)

    preprocessor: ColumnTransformer = model.named_steps["preprocessor"]
    classifier: LogisticRegression = model.named_steps["classifier"]

    summary = {
        "classifier": "LogisticRegression",
        "classifier_params": classifier.get_params(deep=False),
        "numeric_features": NUMERIC_FEATURES,
        "categorical_features": CATEGORICAL_FEATURES,
        "text_feature": TEXT_FEATURE,
        "tfidf_params": preprocessor.named_transformers_["text"].get_params(),
    }
    return model, summary


def compute_evaluation_outputs(model: Pipeline, test_data: pd.DataFrame) -> Dict:
    features = test_data.drop(columns=[LABEL_COLUMN])
    labels = test_data[LABEL_COLUMN]

    predictions = model.predict(features)
    probabilities = model.predict_proba(features)

    evaluation_metrics = {
        "accuracy": accuracy_score(labels, predictions),
        "precision_weighted": precision_score(
            labels, predictions, average="weighted", zero_division=0
        ),
        "recall_weighted": recall_score(
            labels, predictions, average="weighted", zero_division=0
        ),
        "f1_weighted": f1_score(
            labels, predictions, average="weighted", zero_division=0
        ),
    }

    report = classification_report(
        labels,
        predictions,
        output_dict=True,
        zero_division=0,
    )
    class_labels = list(model.named_steps["classifier"].classes_)
    conf_matrix = confusion_matrix(labels, predictions, labels=class_labels)

    probabilities_df = pd.DataFrame(
        probabilities,
        columns=[f"proba_{cls}" for cls in class_labels],
    )
    probabilities_df.insert(0, "prediction", predictions)
    probabilities_df.insert(0, LABEL_COLUMN, labels.reset_index(drop=True))

    return {
        "metrics": evaluation_metrics,
        "predictions": predictions,
        "probabilities": probabilities_df,
        "classification_report": report,
        "confusion_matrix": conf_matrix,
        "class_labels": class_labels,
    }


def flatten_metrics(nested: Dict, prefix: Iterable[str] | None = None) -> Dict[str, float]:
    """Convert nested metric dictionaries into a flat dict for MLflow logging."""
    flat: Dict[str, float] = {}
    current_prefix = list(prefix) if prefix else []

    for key, value in nested.items():
        new_prefix = current_prefix + [key]
        if isinstance(value, dict):
            flat.update(flatten_metrics(value, new_prefix))
        else:
            try:
                flat_key = ".".join(new_prefix)
                flat[flat_key] = float(value)
            except (ValueError, TypeError):
                continue
    return flat


def save_confusion_matrix_figure(
    matrix: np.ndarray,
    output_path: Path,
    labels: tuple[str, ...],
) -> None:
    fig, ax = plt.subplots(figsize=(5, 4))
    cmap = plt.cm.Blues
    ax.imshow(matrix, interpolation="nearest", cmap=cmap)
    ax.set_title("Confusion Matrix")
    tick_marks = np.arange(len(labels))
    ax.set_xticks(tick_marks, labels, rotation=45)
    ax.set_yticks(tick_marks, labels)

    thresh = matrix.max() / 2 if matrix.max() else 0.5
    for i in range(matrix.shape[0]):
        for j in range(matrix.shape[1]):
            ax.text(
                j,
                i,
                f"{matrix[i, j]}",
                ha="center",
                va="center",
                color="white" if matrix[i, j] > thresh else "black",
            )

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    fig.tight_layout()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150)
    plt.close(fig)


def dataframe_to_csv(df: pd.DataFrame, output_path: Path) -> Path:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    return output_path


def log_run_to_mlflow(
    args: argparse.Namespace,
    train_data: pd.DataFrame,
    test_data: pd.DataFrame,
    model: Pipeline,
    model_summary: Dict,
    evaluation_outputs: Dict,
    artifact_root: Path,
) -> None:
    if args.tracking_uri:
        mlflow.set_tracking_uri(args.tracking_uri)

    mlflow.set_experiment(args.experiment_name)

    run_name = args.run_name or f"logreg-{uuid.uuid4().hex[:8]}"
    with mlflow.start_run(run_name=run_name):
        mlflow.log_params(
            {
                "samples": len(train_data) + len(test_data),
                "test_size": args.test_size,
                "random_state": args.random_state,
                "label_column": LABEL_COLUMN,
                "tfidf_max_features": args.tfidf_max_features,
                "logreg_c": args.logreg_c,
            }
        )

        mlflow.log_metrics(evaluation_outputs["metrics"])
        mlflow.log_metrics(
            flatten_metrics(
                evaluation_outputs["classification_report"], prefix=["cls_report"]
            )
        )

        artifacts_dir = artifact_root
        artifacts_dir.mkdir(parents=True, exist_ok=True)

        matrix_path = artifacts_dir / "confusion_matrix.png"
        save_confusion_matrix_figure(
            evaluation_outputs["confusion_matrix"],
            matrix_path,
            labels=tuple(str(label) for label in evaluation_outputs["class_labels"]),
        )
        mlflow.log_artifact(str(matrix_path))

        probabilities_path = dataframe_to_csv(
            evaluation_outputs["probabilities"],
            artifacts_dir / "predicted_probabilities.csv",
        )
        mlflow.log_artifact(str(probabilities_path))

        report_path = artifacts_dir / "classification_report.json"
        report_path.write_text(
            json.dumps(evaluation_outputs["classification_report"], indent=2)
        )
        mlflow.log_artifact(str(report_path))

        summary_path = artifacts_dir / "model_summary.json"
        summary_path.write_text(json.dumps(model_summary, indent=2, default=str))
        mlflow.log_artifact(str(summary_path))

        model_path = artifacts_dir / "trained_pipeline.joblib"
        dump(model, model_path)
        mlflow.log_artifact(str(model_path))


def main() -> None:
    args = parse_args()
    rng = ensure_reproducibility(args.random_state)

    synthetic_df = synthesize_customer_data(args.samples, rng)
    train_df, test_df = prepare_train_test_split(
        synthetic_df, test_size=args.test_size, random_state=args.random_state
    )

    model, model_summary = train_model(
        train_df,
        tfidf_max_features=args.tfidf_max_features,
        logreg_c=args.logreg_c,
        random_state=args.random_state,
    )

    evaluation_outputs = compute_evaluation_outputs(model, test_df)

    artifact_root = Path(args.artifact_dir)
    log_run_to_mlflow(
        args,
        train_df,
        test_df,
        model,
        model_summary,
        evaluation_outputs,
        artifact_root=artifact_root,
    )

    print("Evaluation metrics:", json.dumps(evaluation_outputs["metrics"], indent=2))
    print(f"Artifacts stored under: {artifact_root.resolve()}")


if __name__ == "__main__":
    main()
