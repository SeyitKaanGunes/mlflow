"""
AutoGluon multimodal demo with MLflow tracking.

This script synthesizes a small dataset that mixes numerical, categorical,
and free-form text features. It then trains an AutoGluon MultiModalPredictor
on the data, evaluates the model, and records the entire experiment in MLflow,
including metrics, parameters, artefacts, and the trained model directory.

Example:
    python main.py --samples 400 --time-limit 120 --experiment-name DemoRun

Requirements:
    pip install autogluon.multimodal mlflow scikit-learn matplotlib pandas numpy
"""

from __future__ import annotations

import argparse
import json
import uuid
from pathlib import Path
from typing import Dict, Iterable, Tuple
import warnings

import matplotlib

# Use a non-interactive backend so the script works in headless environments.
matplotlib.use("Agg")

import matplotlib.pyplot as plt
import mlflow
import numpy as np
import pandas as pd
from autogluon.multimodal import MultiModalPredictor
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split


RANDOM_STATE_DEFAULT = 42
LABEL_COLUMN = "churn"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Train an AutoGluon multimodal model on a synthetic dataset and "
            "log the full run into MLflow."
        )
    )
    parser.add_argument(
        "--samples",
        type=int,
        default=400,
        help="Total number of synthetic samples to generate.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.25,
        help="Fraction of samples reserved for evaluation (0, 1).",
    )
    parser.add_argument(
        "--time-limit",
        type=int,
        default=120,
        help="Optional time limit (in seconds) passed to AutoGluon.",
    )
    parser.add_argument(
        "--presets",
        type=str,
        default="default",
        help="AutoGluon presets configuration string (e.g. default, multimodal_fusion, multilingual).",
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default="autogluon-multimodal-demo",
        help="MLflow experiment name.",
    )
    parser.add_argument(
        "--tracking-uri",
        type=str,
        default=None,
        help="MLflow tracking URI. Defaults to local file store.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Custom MLflow run name. Defaults to autogenerated value.",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=RANDOM_STATE_DEFAULT,
        help="Global random seed used for reproducibility.",
    )
    parser.add_argument(
        "--artifact-dir",
        type=str,
        default="artifacts",
        help="Directory used to stash figures and reports before MLflow logging.",
    )
    parser.add_argument(
        "--text-backbone",
        type=str,
        default="sentence-transformers/all-MiniLM-L6-v2",
        help=(
            "HuggingFace checkpoint leveraged for text features by AutoGluon. "
            "Overrides the default backbone."
        ),
    )
    return parser.parse_args()


def ensure_reproducibility(seed: int) -> np.random.Generator:
    np.random.seed(seed)
    return np.random.default_rng(seed)


def synthesize_customer_data(
    num_samples: int,
    rng: np.random.Generator,
) -> pd.DataFrame:
    """Create a toy multimodal dataset with numerical, categorical, and text data."""
    age = rng.integers(18, 70, size=num_samples)
    tenure_months = rng.integers(1, 120, size=num_samples)
    monthly_spend = np.clip(rng.normal(loc=95, scale=25, size=num_samples), 10, None)

    regions = np.array(["North", "South", "East", "West", "Central"])
    customer_segments = np.array(["retail", "enterprise", "startup"])
    preferred_products = np.array(
        ["Analytics", "CRM", "Automation", "Security", "Integration"]
    )

    region = rng.choice(regions, size=num_samples, replace=True)
    segment = rng.choice(customer_segments, size=num_samples, replace=True)
    product = rng.choice(preferred_products, size=num_samples, replace=True)

    # Base propensity calculation encourages AutoGluon to pick up multimodal signals.
    spend_score = (110 - monthly_spend) / 45
    age_score = (35 - age) / 20
    tenure_penalty = (12 - tenure_months) / 18
    segment_bonus = np.isin(segment, ["startup"]).astype(float) * 0.6
    product_bonus = np.isin(product, ["Automation", "Integration"]).astype(float) * 0.5
    churn_logit = (
        -0.3 + spend_score + age_score + tenure_penalty + segment_bonus + product_bonus
    )

    probabilities = 1 / (1 + np.exp(-churn_logit))
    churn_flag = rng.random(num_samples) < probabilities

    text_description = [
        (
            f"{segment[idx].title()} customer from {region[idx]} region with "
            f"{tenure_months[idx]} months on contract spends roughly "
            f"${monthly_spend[idx]:.0f} per month and prioritizes {product[idx]}."
        )
        for idx in range(num_samples)
    ]

    df = pd.DataFrame(
        {
            "age": age,
            "tenure_months": tenure_months,
            "monthly_spend": monthly_spend,
            "region": region,
            "customer_segment": segment,
            "favorite_product": product,
            "interaction_summary": text_description,
            LABEL_COLUMN: np.where(churn_flag, "yes", "no"),
        }
    )
    return df


def prepare_train_test_split(
    df: pd.DataFrame,
    test_size: float,
    random_state: int,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df[LABEL_COLUMN],
    )
    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)


PRESET_ALIAS_MAP = {
    "medium_quality_faster_train": "default",  # legacy AutoGluon preset name
}


def normalize_presets(presets: str) -> str:
    """Map legacy preset names to the current AutoGluon equivalents."""
    return PRESET_ALIAS_MAP.get(presets, presets)


def train_multimodal_model(
    train_data: pd.DataFrame,
    time_limit: int,
    presets: str,
    random_state: int,
    text_backbone: str,
) -> Tuple[MultiModalPredictor, Dict]:
    predictor = MultiModalPredictor(
        label=LABEL_COLUMN,
        problem_type="binary",
        eval_metric="accuracy",
    )

    sanitized_presets = normalize_presets(presets)
    hyperparameters = {
        "model.hf_text.checkpoint_name": text_backbone,
    }

    try:
        predictor.fit(
            train_data=train_data,
            presets=sanitized_presets,
            time_limit=time_limit,
            seed=random_state,
            hyperparameters=hyperparameters,
        )
    except ValueError as exc:
        if "Unknown preset type" in str(exc):
            raise ValueError(
                f"AutoGluon does not recognize the preset '{presets}'. "
                "Try one of the documented presets such as 'default', 'multimodal_fusion', or "
                "'multilingual'. You can override the CLI flag with --presets."
            ) from exc
        raise

    fit_summary = predictor.fit_summary()
    if isinstance(fit_summary, dict):
        fit_summary["custom_text_backbone"] = text_backbone
        fit_summary["applied_hyperparameters"] = hyperparameters
    return predictor, fit_summary


def compute_evaluation_outputs(
    predictor: MultiModalPredictor,
    test_data: pd.DataFrame,
) -> Dict:
    evaluation_metrics = predictor.evaluate(test_data)
    predictions = predictor.predict(test_data)
    probabilities = predictor.predict_proba(test_data)
    report = classification_report(
        test_data[LABEL_COLUMN], predictions, output_dict=True, zero_division=0
    )
    class_labels = (
        predictor.class_labels if hasattr(predictor, "class_labels") else sorted(test_data[LABEL_COLUMN].unique())
    )
    conf_matrix = confusion_matrix(test_data[LABEL_COLUMN], predictions, labels=class_labels)

    return {
        "metrics": evaluation_metrics,
        "predictions": predictions,
        "probabilities": probabilities,
        "classification_report": report,
        "confusion_matrix": conf_matrix,
        "class_labels": class_labels,
    }


def flatten_metrics(nested: Dict, prefix: Iterable[str] | None = None) -> Dict[str, float]:
    """Convert nested metric dictionaries into a flat dict for MLflow logging."""
    flat: Dict[str, float] = {}
    current_prefix = list(prefix) if prefix else []

    for key, value in nested.items():
        new_prefix = current_prefix + [key]
        if isinstance(value, dict):
            flat.update(flatten_metrics(value, new_prefix))
        else:
            try:
                flat_key = ".".join(new_prefix)
                flat[flat_key] = float(value)
            except (ValueError, TypeError):
                continue
    return flat


def save_confusion_matrix_figure(
    matrix: np.ndarray,
    output_path: Path,
    labels: Tuple[str, ...],
) -> None:
    fig, ax = plt.subplots(figsize=(5, 4))
    cmap = plt.cm.Blues
    ax.imshow(matrix, interpolation="nearest", cmap=cmap)
    ax.set_title("Confusion Matrix")
    tick_marks = np.arange(len(labels))
    ax.set_xticks(tick_marks, labels, rotation=45)
    ax.set_yticks(tick_marks, labels)

    thresh = matrix.max() / 2 if matrix.max() else 0.5
    for i in range(matrix.shape[0]):
        for j in range(matrix.shape[1]):
            ax.text(
                j,
                i,
                f"{matrix[i, j]}",
                ha="center",
                va="center",
                color="white" if matrix[i, j] > thresh else "black",
            )

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    fig.tight_layout()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150)
    plt.close(fig)


def dataframe_to_csv(df: pd.DataFrame, output_path: Path) -> Path:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    return output_path


def log_run_to_mlflow(
    args: argparse.Namespace,
    train_data: pd.DataFrame,
    test_data: pd.DataFrame,
    predictor: MultiModalPredictor,
    fit_summary: Dict,
    evaluation_outputs: Dict,
    artifact_root: Path,
) -> None:
    if args.tracking_uri:
        mlflow.set_tracking_uri(args.tracking_uri)

    mlflow.set_experiment(args.experiment_name)

    run_name = args.run_name or f"autogluon-multimodal-{uuid.uuid4().hex[:8]}"
    with mlflow.start_run(run_name=run_name):
        mlflow.log_params(
            {
                "samples": len(train_data) + len(test_data),
                "test_size": args.test_size,
                "time_limit": args.time_limit,
                "presets": args.presets,
                "random_state": args.random_state,
                "label_column": LABEL_COLUMN,
                "text_backbone": args.text_backbone,
            }
        )

        mlflow.log_metrics(evaluation_outputs["metrics"])
        mlflow.log_metrics(flatten_metrics(evaluation_outputs["classification_report"], prefix=["cls_report"]))

        artifacts_dir = artifact_root
        artifacts_dir.mkdir(parents=True, exist_ok=True)

        # Persist evaluation artefacts for later inspection.
        matrix_path = artifacts_dir / "confusion_matrix.png"
        save_confusion_matrix_figure(
            evaluation_outputs["confusion_matrix"],
            matrix_path,
            labels=tuple(str(label) for label in evaluation_outputs["class_labels"]),
        )
        mlflow.log_artifact(str(matrix_path))

        if hasattr(predictor, "leaderboard"):
            try:
                leaderboard_df = predictor.leaderboard(test_data, silent=True)
            except TypeError:
                leaderboard_df = predictor.leaderboard(silent=True)
            leaderboard_path = dataframe_to_csv(
                leaderboard_df, artifacts_dir / "leaderboard.csv"
            )
            mlflow.log_artifact(str(leaderboard_path))
        else:
            warnings.warn(
                "Current AutoGluon MultiModalPredictor version does not expose a leaderboard; skipping leaderboard logging.",
                stacklevel=2,
            )

        proba_path = dataframe_to_csv(
            evaluation_outputs["probabilities"],
            artifacts_dir / "predicted_probabilities.csv",
        )
        mlflow.log_artifact(str(proba_path))

        report_path = artifacts_dir / "classification_report.json"
        report_path.write_text(json.dumps(evaluation_outputs["classification_report"], indent=2))
        mlflow.log_artifact(str(report_path))

        summary_path = artifacts_dir / "fit_summary.json"
        summary_path.write_text(json.dumps(fit_summary, indent=2, default=str))
        mlflow.log_artifact(str(summary_path))

        # Save the trained model directory so the run is fully reproducible.
        model_path = artifacts_dir / "multimodal_predictor"
        predictor.save(str(model_path))
        mlflow.log_artifacts(str(model_path))


def main() -> None:
    args = parse_args()
    rng = ensure_reproducibility(args.random_state)

    synthetic_df = synthesize_customer_data(args.samples, rng)
    train_df, test_df = prepare_train_test_split(
        synthetic_df, test_size=args.test_size, random_state=args.random_state
    )

    predictor, fit_summary = train_multimodal_model(
        train_df,
        time_limit=args.time_limit,
        presets=args.presets,
        random_state=args.random_state,
        text_backbone=args.text_backbone,
    )

    evaluation_outputs = compute_evaluation_outputs(predictor, test_df)

    artifact_root = Path(args.artifact_dir)
    log_run_to_mlflow(
        args,
        train_df,
        test_df,
        predictor,
        fit_summary,
        evaluation_outputs,
        artifact_root=artifact_root,
    )

    # Surface key results to the console for convenient inspection.
    print("Evaluation metrics:", json.dumps(evaluation_outputs["metrics"], indent=2))
    print(f"Artifacts stored under: {artifact_root.resolve()}")


if __name__ == "__main__":
    main()
